{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTbuzVh5l3rtHhZ0pLzlF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/who0liebo0lie/ArticleTraffic/blob/main/Scrape_%26_Evaluate_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1ek215dX_1P",
        "outputId": "48bc17ac-62fe-48f5-da82-304c11c326fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas nltk\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to download NLTK data\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    NLTK_AVAILABLE = True\n",
        "except:\n",
        "    print(\"NLTK not available - using simple sentence splitting\")\n",
        "    NLTK_AVAILABLE = False\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class JWStruggleAnalysisScraper:\n",
        "    def __init__(self, base_url=\"https://wol.jw.org\"):\n",
        "        self.base_url = base_url\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "\n",
        "        # Struggle indicators\n",
        "        self.struggle_indicators = [\n",
        "            'struggled', 'difficulty', 'challenge', 'problem', 'issue', 'obstacle',\n",
        "            'hardship', 'trial', 'suffering', 'pain', 'hurt', 'difficult',\n",
        "            'troubled', 'worried', 'anxious', 'depressed', 'overwhelmed',\n",
        "            'addiction', 'abuse', 'divorce', 'death', 'illness', 'financial',\n",
        "            'unemployment', 'rejection', 'loneliness', 'fear', 'doubt',\n",
        "            'temptation', 'persecution', 'opposition', 'criticism', 'failure',\n",
        "            'lost', 'confused', 'helpless', 'hopeless', 'discouraged',\n",
        "            'disappointed', 'frustrated', 'angry', 'bitter', 'resentful'\n",
        "        ]\n",
        "\n",
        "        # Struggle categories\n",
        "        self.struggle_categories = {\n",
        "            'health_mental': ['depression', 'anxiety', 'mental', 'emotional', 'psychological', 'stressed', 'overwhelmed', 'panic', 'bipolar', 'ptsd'],\n",
        "            'health_physical': ['illness', 'disease', 'cancer', 'disability', 'chronic', 'pain', 'medical', 'surgery', 'accident', 'injury'],\n",
        "            'addiction': ['alcohol', 'drugs', 'gambling', 'smoking', 'substance', 'addiction', 'addicted', 'dependency'],\n",
        "            'relationships': ['divorce', 'marriage', 'family', 'spouse', 'children', 'parents', 'relationship', 'domestic', 'abuse', 'cheating'],\n",
        "            'financial': ['money', 'financial', 'debt', 'unemployment', 'job', 'poverty', 'homeless', 'foreclosure', 'bankruptcy'],\n",
        "            'spiritual': ['faith', 'doubt', 'belief', 'god', 'prayer', 'bible', 'congregation', 'disfellowship', 'guilt', 'sin'],\n",
        "            'social': ['rejection', 'persecution', 'discrimination', 'bullying', 'loneliness', 'isolation', 'friends', 'social'],\n",
        "            'loss_grief': ['death', 'died', 'funeral', 'grief', 'mourning', 'loss', 'bereaved', 'widow', 'orphan'],\n",
        "            'education_career': ['school', 'education', 'career', 'work', 'college', 'university', 'study', 'academic', 'professional'],\n",
        "            'identity_purpose': ['identity', 'purpose', 'meaning', 'direction', 'confused', 'lost', 'self-worth', 'confidence']\n",
        "        }\n",
        "\n",
        "    def get_page_content(self, url):\n",
        "        \"\"\"Fetch and parse webpage content\"\"\"\n",
        "        try:\n",
        "            print(f\"  Fetching: {url}\")\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            return BeautifulSoup(response.content, 'html.parser')\n",
        "        except Exception as e:\n",
        "            print(f\"  Error fetching {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def simple_sentence_split(self, text):\n",
        "        \"\"\"Simple sentence splitting when NLTK is not available\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "\n",
        "    def extract_personal_experiences(self, soup):\n",
        "        \"\"\"Extract sections that contain personal experiences and struggles\"\"\"\n",
        "        experience_data = {\n",
        "            'struggle_sentences': [],\n",
        "            'personal_stories': [],\n",
        "            'quotes': []\n",
        "        }\n",
        "\n",
        "        # Get all text content\n",
        "        full_text = soup.get_text()\n",
        "\n",
        "        # Split into sentences\n",
        "        if NLTK_AVAILABLE:\n",
        "            try:\n",
        "                sentences = sent_tokenize(full_text)\n",
        "            except:\n",
        "                sentences = self.simple_sentence_split(full_text)\n",
        "        else:\n",
        "            sentences = self.simple_sentence_split(full_text)\n",
        "\n",
        "        # Find struggle sentences\n",
        "        for sentence in sentences:\n",
        "            if len(sentence) < 20:\n",
        "                continue\n",
        "\n",
        "            sentence_lower = sentence.lower()\n",
        "\n",
        "            # Check for struggle indicators\n",
        "            for indicator in self.struggle_indicators:\n",
        "                if indicator in sentence_lower:\n",
        "                    experience_data['struggle_sentences'].append({\n",
        "                        'sentence': sentence,\n",
        "                        'indicator': indicator\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "        # Extract quotes\n",
        "        quotes = soup.find_all(['blockquote', 'q'])\n",
        "        for quote in quotes:\n",
        "            quote_text = quote.get_text(strip=True)\n",
        "            if len(quote_text) > 30:\n",
        "                quote_lower = quote_text.lower()\n",
        "\n",
        "                struggle_indicators_found = []\n",
        "                for indicator in self.struggle_indicators:\n",
        "                    if indicator in quote_lower:\n",
        "                        struggle_indicators_found.append(indicator)\n",
        "\n",
        "                if struggle_indicators_found:\n",
        "                    experience_data['quotes'].append({\n",
        "                        'text': quote_text,\n",
        "                        'indicators': struggle_indicators_found\n",
        "                    })\n",
        "\n",
        "        # Find first-person narratives\n",
        "        first_person_patterns = [\n",
        "            r'I (was|am|had|have|felt|experienced|struggled|faced)',\n",
        "            r'My (life|family|marriage|health|faith|experience)',\n",
        "            r'When I (was|felt|experienced|faced|struggled)'\n",
        "        ]\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence) < 20:\n",
        "                continue\n",
        "\n",
        "            for pattern in first_person_patterns:\n",
        "                if re.search(pattern, sentence, re.IGNORECASE):\n",
        "                    sentence_lower = sentence.lower()\n",
        "                    for indicator in self.struggle_indicators:\n",
        "                        if indicator in sentence_lower:\n",
        "                            experience_data['personal_stories'].append({\n",
        "                                'sentence': sentence,\n",
        "                                'indicator': indicator\n",
        "                            })\n",
        "                            break\n",
        "                    break\n",
        "\n",
        "        return experience_data\n",
        "\n",
        "    def categorize_struggles(self, experience_data):\n",
        "        \"\"\"Categorize struggles into predefined categories\"\"\"\n",
        "        categorized = {category: [] for category in self.struggle_categories.keys()}\n",
        "        categorized['uncategorized'] = []\n",
        "\n",
        "        all_experiences = (\n",
        "            experience_data['struggle_sentences'] +\n",
        "            experience_data['personal_stories'] +\n",
        "            experience_data['quotes']\n",
        "        )\n",
        "\n",
        "        for experience in all_experiences:\n",
        "            text = experience.get('sentence', experience.get('text', ''))\n",
        "            text_lower = text.lower()\n",
        "            categorized_flag = False\n",
        "\n",
        "            for category, keywords in self.struggle_categories.items():\n",
        "                for keyword in keywords:\n",
        "                    if keyword in text_lower:\n",
        "                        categorized[category].append({\n",
        "                            'text': text,\n",
        "                            'matched_keyword': keyword,\n",
        "                            'original_data': experience\n",
        "                        })\n",
        "                        categorized_flag = True\n",
        "                        break\n",
        "                if categorized_flag:\n",
        "                    break\n",
        "\n",
        "            if not categorized_flag:\n",
        "                categorized['uncategorized'].append({\n",
        "                    'text': text,\n",
        "                    'original_data': experience\n",
        "                })\n",
        "\n",
        "        return categorized\n",
        "\n",
        "    def extract_article_struggles(self, article_url):\n",
        "        \"\"\"Extract struggles from a single article\"\"\"\n",
        "        soup = self.get_page_content(article_url)\n",
        "        if not soup:\n",
        "            return None\n",
        "\n",
        "        article_data = {\n",
        "            'url': article_url,\n",
        "            'scraped_at': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Extract title\n",
        "        title = soup.find('h1')\n",
        "        if title:\n",
        "            article_data['title'] = title.get_text(strip=True)\n",
        "        else:\n",
        "            article_data['title'] = 'Unknown Title'\n",
        "\n",
        "        # Extract experiences\n",
        "        experiences = self.extract_personal_experiences(soup)\n",
        "        article_data['experiences'] = experiences\n",
        "\n",
        "        # Categorize struggles\n",
        "        categorized = self.categorize_struggles(experiences)\n",
        "        article_data['struggle_categories'] = categorized\n",
        "\n",
        "        # Count total struggles\n",
        "        total_struggles = sum(len(struggles) for struggles in categorized.values())\n",
        "        article_data['total_struggles'] = total_struggles\n",
        "\n",
        "        return article_data\n",
        "\n",
        "    def get_article_list(self, url):\n",
        "        \"\"\"Get article URLs from main page\"\"\"\n",
        "        soup = self.get_page_content(url)\n",
        "        if not soup:\n",
        "            return []\n",
        "\n",
        "        articles = []\n",
        "\n",
        "        # Find article links\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "\n",
        "        for link in all_links:\n",
        "            try:\n",
        "                article_title = link.get_text(strip=True)\n",
        "                article_url = urljoin(self.base_url, link['href'])\n",
        "\n",
        "                # Basic filtering\n",
        "                if (len(article_title) > 10 and\n",
        "                    \"from our readers\" not in article_title.lower() and\n",
        "                    '/wol/d/' in article_url):\n",
        "\n",
        "                    articles.append({\n",
        "                        'title': article_title,\n",
        "                        'url': article_url\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Remove duplicates\n",
        "        seen_urls = set()\n",
        "        unique_articles = []\n",
        "        for article in articles:\n",
        "            if article['url'] not in seen_urls:\n",
        "                seen_urls.add(article['url'])\n",
        "                unique_articles.append(article)\n",
        "\n",
        "        return unique_articles[:50]  # Limit to first 50 for safety\n",
        "\n",
        "    def scrape_struggles(self, url, max_articles=5):\n",
        "        \"\"\"Main scraping function\"\"\"\n",
        "        print(f\"Starting scrape of {url}\")\n",
        "        print(f\"Maximum articles: {max_articles}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Get article list\n",
        "        print(\"Getting article list...\")\n",
        "        articles = self.get_article_list(url)\n",
        "\n",
        "        if not articles:\n",
        "            print(\"No articles found!\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Found {len(articles)} potential articles\")\n",
        "\n",
        "        # Limit articles\n",
        "        if max_articles:\n",
        "            articles = articles[:max_articles]\n",
        "\n",
        "        # Scrape each article\n",
        "        dataset = []\n",
        "        for i, article in enumerate(articles, 1):\n",
        "            print(f\"\\nArticle {i}/{len(articles)}: {article['title'][:60]}...\")\n",
        "\n",
        "            struggle_data = self.extract_article_struggles(article['url'])\n",
        "\n",
        "            if struggle_data and struggle_data['total_struggles'] > 0:\n",
        "                dataset.append(struggle_data)\n",
        "                print(f\"  ✓ Found {struggle_data['total_struggles']} struggles\")\n",
        "            else:\n",
        "                print(f\"  - No struggles found\")\n",
        "\n",
        "            # Delay between requests\n",
        "            time.sleep(2)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"SCRAPING COMPLETE\")\n",
        "        print(f\"Articles processed: {len(articles)}\")\n",
        "        print(f\"Articles with struggles: {len(dataset)}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        return dataset\n"
      ],
      "metadata": {
        "id": "f191EOm-YKE-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the scraper\n",
        "url = \"https://wol.jw.org/en/wol/d/r1/lp-e/1200273453\"\n",
        "max_articles = 8  # Start small for testing\n",
        "\n",
        "# Create scraper and run\n",
        "scraper = JWStruggleAnalysisScraper()\n",
        "dataset = scraper.scrape_struggles(url, max_articles=max_articles)\n",
        "\n",
        "# CELL 4: Process Results\n",
        "if dataset:\n",
        "    print(\"\\nProcessing results...\")\n",
        "\n",
        "    # Create analysis dataset\n",
        "    analysis_data = []\n",
        "    category_counts = {}\n",
        "\n",
        "    for article in dataset:\n",
        "        for category, struggles in article['struggle_categories'].items():\n",
        "            if category not in category_counts:\n",
        "                category_counts[category] = 0\n",
        "            category_counts[category] += len(struggles)\n",
        "\n",
        "            for struggle in struggles:\n",
        "                analysis_data.append({\n",
        "                    'article_title': article['title'],\n",
        "                    'article_url': article['url'],\n",
        "                    'struggle_category': category,\n",
        "                    'struggle_text': struggle['text'],\n",
        "                    'matched_keyword': struggle.get('matched_keyword', ''),\n",
        "                    'text_length': len(struggle['text']),\n",
        "                    'scraped_at': article['scraped_at']\n",
        "                })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(analysis_data)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nRESULTS SUMMARY:\")\n",
        "    print(f\"Total articles with struggles: {len(dataset)}\")\n",
        "    print(f\"Total struggle instances: {len(df)}\")\n",
        "    print(f\"Average struggles per article: {len(df)/len(dataset):.1f}\")\n",
        "\n",
        "    print(f\"\\nCATEGORY DISTRIBUTION:\")\n",
        "    for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        if count > 0:\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Save files\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    json_filename = f'jw_struggles_{timestamp}.json'\n",
        "    csv_filename = f'jw_struggles_{timestamp}.csv'\n",
        "\n",
        "    # Save JSON\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Save CSV\n",
        "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"\\nFiles saved:\")\n",
        "    print(f\"  {json_filename}\")\n",
        "    print(f\"  {csv_filename}\")\n",
        "\n",
        "    # Show sample data\n",
        "    print(f\"\\nSAMPLE DATA:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(f\"\\nDataset ready for analysis!\")\n",
        "\n",
        "else:\n",
        "    print(\"No data collected. Try adjusting the URL or parameters.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN9NRH-vYKHy",
        "outputId": "9b091939-d79e-443a-a626-9de1709d0ffd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scrape of https://wol.jw.org/en/wol/d/r1/lp-e/1200273453\n",
            "Maximum articles: 8\n",
            "--------------------------------------------------\n",
            "Getting article list...\n",
            "  Fetching: https://wol.jw.org/en/wol/d/r1/lp-e/1200273453\n",
            "Found 4 potential articles\n",
            "\n",
            "Article 1/4: List by Name...\n",
            "  Fetching: https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#p9\n",
            "  ✓ Found 29 struggles\n",
            "\n",
            "Article 2/4: List by Title...\n",
            "  Fetching: https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#p621\n",
            "  ✓ Found 29 struggles\n",
            "\n",
            "Article 3/4: List by Name...\n",
            "  Fetching: https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#h=9\n",
            "  ✓ Found 29 struggles\n",
            "\n",
            "Article 4/4: List by Title...\n",
            "  Fetching: https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#h=621\n",
            "  ✓ Found 29 struggles\n",
            "\n",
            "==================================================\n",
            "SCRAPING COMPLETE\n",
            "Articles processed: 4\n",
            "Articles with struggles: 4\n",
            "==================================================\n",
            "\n",
            "Processing results...\n",
            "\n",
            "RESULTS SUMMARY:\n",
            "Total articles with struggles: 4\n",
            "Total struggle instances: 116\n",
            "Average struggles per article: 29.0\n",
            "\n",
            "CATEGORY DISTRIBUTION:\n",
            "  relationships: 56 (48.3%)\n",
            "  spiritual: 28 (24.1%)\n",
            "  health_physical: 8 (6.9%)\n",
            "  addiction: 8 (6.9%)\n",
            "  loss_grief: 8 (6.9%)\n",
            "  education_career: 4 (3.4%)\n",
            "  uncategorized: 4 (3.4%)\n",
            "\n",
            "Files saved:\n",
            "  jw_struggles_20250703_175239.json\n",
            "  jw_struggles_20250703_175239.csv\n",
            "\n",
            "SAMPLE DATA:\n",
            "   article_title                                        article_url  \\\n",
            "0  Unknown Title  https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#p9   \n",
            "1  Unknown Title  https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#p9   \n",
            "2  Unknown Title  https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#p9   \n",
            "3  Unknown Title  https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#p9   \n",
            "4  Unknown Title  https://wol.jw.org/en/wol/d/r1/lp-e/1200273453#p9   \n",
            "\n",
            "  struggle_category                                      struggle_text  \\\n",
            "0   health_physical  04 23-26\\nForsaken Orphan Finds a Loving Fathe...   \n",
            "1   health_physical  04 23-26\\nForsaken Orphan Finds a Loving Fathe...   \n",
            "2         addiction  , We Do Not Give Up”: w95 2/1 20-25\\nSinging C...   \n",
            "3         addiction  , We Do Not Give Up”: w95 2/1 20-25\\nSinging C...   \n",
            "4     relationships  10 3-6\\nBlessings “in Favorable Times and Diff...   \n",
            "\n",
            "  matched_keyword  text_length                  scraped_at  \n",
            "0            pain         1982  2025-07-03T17:52:03.406016  \n",
            "1            pain         1982  2025-07-03T17:52:03.406016  \n",
            "2         alcohol         1724  2025-07-03T17:52:03.406016  \n",
            "3         alcohol         1724  2025-07-03T17:52:03.406016  \n",
            "4        children          683  2025-07-03T17:52:03.406016  \n",
            "\n",
            "Dataset ready for analysis!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download files to your computer\n",
        "from google.colab import files\n",
        "files.download(json_filename)\n",
        "files.download(csv_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wR585aZMYKLM",
        "outputId": "bd24f809-6c27-4076-95b0-a1816b62e4c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0699e0ac-d605-4c12-bf2e-84cd4d6ebe96\", \"jw_struggles_20250703_175239.json\", 390436)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d2ceb8a8-0694-4485-8f6b-c5989c26dfbe\", \"jw_struggles_20250703_175239.csv\", 130895)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Load and clean the data\n",
        "import pandas as pd\n",
        "df = pd.read_csv('jw_struggles_20250703_175239')\n",
        "\n",
        "# Remove duplicates and clean text\n",
        "df = df.drop_duplicates(subset=['struggle_text'])\n",
        "df['struggle_text'] = df['struggle_text'].str.strip()"
      ],
      "metadata": {
        "id": "5dNcFr6gYKOl"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}